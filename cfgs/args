[env-module]
    help=Environment module(s). Each module `env_module` from env_modules is located at envs/`env_module`/env.py and has to encapsulate a class for which the name has to be `env_module` with first capitalized letter (e.g. Foo will be extracted from envs/foo/env.py)
    required=True
    dest=env_module
[env-conf-file]
    help=Environment configuration file. A configuration file for an environment encapsulated in `env_module` has to be located at at confs/conf_env/`env_module`/`name` where `name` is your configuration file
    dest=env_conf_file
    default=DEFAULT
[max-size-episode]
    help=Maximum size of each episode. Usage may vary among runners.
    type=int
    default=50
    dest=max_size_episode
[only-full-history]
    help=If activated, when there is an history, only train with data that contains a full history. Otherwise, fill with zeroes missing part of the history
    type=bool
    default=True
    dest=only_full_history
[replay-memory-size]
    help=Replay memory size of the agent
    type=int
    default=1000000
    dest=replay_memory_size
[exp-priority]
    help=0 for uniform experience replay, greater is how much priority is used
    type=float
    default=0
    dest=exp_priority
[out-prefix]
    help=Prefix of output file
    dest=out_prefix
    default=""
[rng]
    help=Random number generator seed. Chosen randomly when empty.
    type=int
    default=-1
    dest=rng
[backend-nnet]
    help=Neural network backend binded to neural network controller
    default=n_deerfault
    dest=backend_nnet
[backend-nnet-args]
    help=Args for the neural network backend. 1st arg may be either a configuration file or a 'key=value' argument
    dest=backend_nnet_conf_file
    nargs=*
    action=append
[batch-size]
    help=Batch size for qlearning training phase
    type=int
    default=32
    dest=batch_size
[epochs]
    help=Number of q-network learning epochs to perform
    type=int
    dest=epochs
    default=10
[qnetw-module]
    help=Neural network controller module
    default=ctrl_deerfault
    dest=qnetw_module
[qnetw-args]
    help=Args for neural network controller. 1st arg may be either a configuration file or a 'key=value' argument
    dest=ctrl_neural_nets_conf_file
    nargs=*
    action=append
[attach]
    help=Controllers to attach. Order matters. First sub argument is the name of the controller, second argument is the configuration file or the first overriding argument, and the remainder is argument overriding
    dest=controllers
    nargs=+
    action=append
[pol-train-module]
    help=Class module of the training policy
    default=epsilonGreedyPolicy
    dest=pol_train_module
[pol-train-args]
    help=Args for the training policy. 1st arg may be either a configuration file or a 'key=value' argument
    dest=pol_train_args
    nargs=*
    action=append
[pol-test-module]
    help=Class module of the test policy
    default=greedyPolicy
    dest=pol_test_module
[pol-test-args]
    help=Args for the testing policy. 1st arg may be either a configuration file or a 'key=value' argument
    dest=pol_test_args
    nargs=*
    action=append
